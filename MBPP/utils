
def check_gpu_memory():
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_de
        allocated_memory = torch.cuda.me
        cached_memory = torch.cuda.memor

        print(f"Total GPU Memory: {total
        print(f"Allocated Memory: {alloc
        print(f"Cached Memory: {cached_m
    else:
        print("No GPU available.")